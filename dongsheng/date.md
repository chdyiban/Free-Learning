日期|工作学习情况
:-:|:-
8.1|今天复习了一下tornado框架和sqlachemy模块，学习使用alembic模块进行数据库迁移
8.2|今天进度可以算无，看了看python里面的上下文管理器，else块，以及刚刚开始看如何将使用协程
8.3|接着看协程，从生成器到协程的转变
8.4|1、今天学习了终止协程和异常处理，以及在协程中返回值</br>2、初步理解了一点yield from这个语言结构使用方法和原理，虽然知道它很强大，但是还是不能理解这么复杂的语法的意义
8.5|今天学习了一个用来处理并发的模块concurrent.futures，主要的两个类ThreadPoolExecutor和ProcessPoolExecutor内部维护着一个线程池或者进程池，感觉很方便，但是实际使用的时候可能封装级别较低的Threading和multiprocessing这两个模块可能更加灵活。
8.6|今日进度：无
8.7|1、今天学习了协程的使用，两个扩展包greenlet和gevent模块，使用协程实现并发</br>2、看了一下python标准库里面的协程asyncio模块，不过感觉很难，暂时还没有看懂。
8.8|今日进度：无
8.9|1、今天继续学习了asyncio模块</br>2、今天对之前线程池，进程池以及线程进程通信做了一个小结
8.10|无
8.11|1、学习了tornado框架中的websocket的使用</br>2、开始学习python的元编程，正在看动态属性和特性
8.12|1、继续学习元编程，创建描述符</br>2、正在学习shell编程
8.13|1、学习创建元类，以及与普通类的关系</br>2、复习了tornado异步的使用
8.14|1、学习shell编程，if，for，while语句，按照视频写了个简单的MySQL数据库自动备份脚本</br>2、学习了jQuery中使用Ajax与tornado进行数据交互
8.15|无
8.16|学习shell编程，学习了until，case，select语句以及数组和函数，学习sed，grep，awk以及find的基本使用
8.17|学习使用Supervisor来监控tornado进程
8.18|1、学习numpy的基本使用</br>2、学习使用pandas进行数据处理
8.19|1、学习pandas里面一些处理数据的操作</br>2、今天开始学习matplotlib
8.20|今日进度：继续学习了一点点matplotlib进行数据可视化
8.21~9.1|进度几乎为无
9.2|今日进度：学习一个对matplotlib进一步封装的模块seaborn
9.3|今日进度：1、测试了一下图书馆，在学校也没办法访问，明天再用校园网试一下</br>2、跟着视频学习了一些线性回归方面的知识
9.4~9.12|今日进度：几乎为无，没怎么记录
9.13|1、今天尝试写了异步代码替换之前使用requests库写的阻塞代码，之前的代码完全没有用到tornado的异步特性，对于性能没有什么提升。</br>2、重新看了看tornado里面关于协程的使用以及python中新加入的关键词async和await
9.15|完成部署，在服务器创建了一个拥有sudo权限的用户py，编译安装了python3.7和一些附件，以及使用python2安装了supervisor监控tornado进程，修改了nginx配置文件，增加了upstream和server用来做tornado的负载均衡。使用python2安装了virtualenv来创建虚拟环境
9.16|无
9.17|1、学习了一些关于代码检查代码测试的内容</br>2、看看了逻辑回归算法的讲解视频
9.18|今日进度：无
9.19|今日进度：折腾了一天的win0子系统上面的Linux，感觉win10的子系统有点坑，入门学Linux还是挺方便的，另外学了一些网络测试命令
9.20|学习基本的设计模式，使用python实现单例模式和mixin模式，在python中应该可以很方便的创建一个可扩展的动态生成不同实例的代码，不过暂时遇到一个小bug，等待解决
9.21|今日进度：跟着视频做了一个梯度下降求解线性回归的案例
9.22|今日进度：看了一遍一个简易版的scrapy的中文翻译，使用爬虫框架感觉简直是一应俱全，很强大很方便。今天主要学习里面的操作流程和css选择器、xpath以及它的全局命令
9.23|1、看scrapy的官方文档，学习了几种不同的爬虫的使用、将爬取的数据写入数据库以及一个非常好用的链接提取器。</br>2、初步分析了一下先锋家园这个网站，感觉很适合练手，比较整齐，可能个别页面需要更改爬取的规则，准备直接用框架将所有板块的链接提取出来，就可以直接爬新闻的链接了，在框架里面感觉想要什么就有什么，提取链接，分页，以及文本内容抓取这些都很方便。
9.24|学习scrapy的文档，今天爬取先锋家园所有版块的链接，然后准备进行进一步的数据抓取，但是似乎在这个地方遇到了一个问题，框架在提取链接的时候自动进行了深度遍历，然后当我希望进行再次爬取的时候会失败，但是当我在请求中设置参数使得可以重复爬取的时候，这个时候又会陷入爬行循环，会重复好几次我所做的工作，或许这是其他失误操作所导致。。。。。。在写今日进度的时候问题刚好解决，但是有些问题还是应该继续深入框架
9.25|今日进度：先锋家园爬虫爬取解析部分代码基本完成，正在增加一个redis的过滤操作和将数据写入MySQL数据库
9.26|写完了先锋家园的爬虫，但是由于最后我的ip被限制了（本来还可以用的，可能测试的次数太多了，被制裁了），估计235和236两个服务器也是由于访问次数过多所以造成的无法访问学校网站。明天写个代理应该就可以了
9.27|无
9.28|先锋家园爬虫已经完成，现在先用我的数据库进行测试，可以了的话就可以直接爬到235的数据库里面了
9.29|在236上面装了个redis服务，调试了一下配置文件以及redis和mysql的连接情况，解决在后台运行爬虫的问题，解决了一个配置日志输出文件的小问题，爬虫正常运行
9.30~10.7|国庆七天没有做什么，深入看了一遍scrapy的文档，各种中间件以及配置（不过感觉看完就忘得差不多了。。），另外在GitHub上面找到了一个爬取代理的项目，还是很不错，用tornado以及scrapy结合写的一个项目，部署到了我的服务器上面，感觉深入理解理解里面tornado和scrapy结合的地方很有好处
10.8|今天将爬虫数据库去重以及字段修改完了，改进了一下代码，还没有进行最终测试
10.9|先锋家园爬虫测试完毕，优化了部分代码，正在将其加入到定时任务中
10.10|无
10.11|1、先锋家园爬虫完毕，但是更改了tag和做了数据库过滤之后感觉效率低了很多。</br>2、信息门户爬虫刚开始，使用scrapy的CookiesMiddleware中间件使用cookie</br>找到了一本不错的书《learning+scrapy》，大概可以找到嵌入web以及合理使用性能的方法
10.12~10.13|1、先锋家园爬虫完毕</br>2、信息门户爬虫完毕</br>3、对于爬虫框架的使用还不够熟练以及对于异步框架twisted使用还不会，代码还需要优化，另外由于可能由于数据库的原因造成效率低，这个地方也需要优化
10.14|1、改了改之前爬虫的bug，一个由于title没有做转义导致的错误和一个由于author太长导致的错误以及在信息门户的新闻有部分会跳转到其他链接，这个直接在框架里面设置为禁止访问了。另外有一个bug，似乎每次爬虫爬取文章数量总是预计的两倍，这个有待解决。</br>2、继续看scrapy的文档和《learning scrapy》，学习使用Item Loader来处理响应，有助于简化代码
10.15|无
10.16|1、看scrapy的文档，学习load item的使用
10.17|学习itemloader，用来提取数据，使得爬取和解析分离，逻辑更加清楚，另外写了一篇笔记。
10.18|正在写爬取学校官网新闻，提取链接。
10.19|写完了官网的爬取和解析部分，数据库表和字段有更改，正在更改与数据库相关的逻辑
10.20|无
10.21|修改了爬虫与数据库交互部分的逻辑，以及一些小bug
10.22|增加了爬虫增量爬取的功能，设置了延迟，爬取的速度降低，但是由于是一直跑着，所以速度影响不大
10.23|1、学校官网爬虫，先锋家园爬虫，信息门户爬虫三个都增加了增量爬取（准确说是实时更新爬取的功能），而且已经正常使用</br>2、接下来准备继续优化当前爬虫代码并且开始阅读异步框架twisted的文档，另外准备开始写易班官网的爬虫
10.24|修改了部分爬虫有问题的地方，还没来得及测试
10.25|解决了爬虫几个小问题，调换了爬取的两个时间，补全文章图片链接，加上了tags表
10.26|改了改爬虫出现的问题，但是或许还是有问题，而且令人感到奇怪的是爬虫爬取的效率比之前差了很多，这个还要继续找原因
10.27|无
10.28|无
10.29|无
10.30|无
10.31|今日进度：想办法让爬虫能够按照顺序插入到数据库，由于框架的限制，无法从调度引擎入手，尝试写了个扩展，用于执行预定好的顺序，暂时还没有成功，用中间件去做可能会更好。
11.1|看了一遍一个关于分布式爬虫的库scrapy-redis的源码，希望能够找到调用scrapy引擎的方法和自定义队列的方法。不过还不是很懂。
11.2|无
11.3|1、优化了以前的两个爬虫</br>2、暂时不准备对scrapy框架进行改造。目前准备直接将所有数据放在内存里面进行排序，然后再插入数据库，这样做的话很慢，不再有异步的优势，不过这是最简单最直接的办法。
11.4|今日进度：优化了信息门户的爬虫
11.5|1、完成了爬虫</br>2、注册了一个七牛云账号，暂时还在认证中，这个接口还没有写，准备等这个好了再重新爬数据
11.6|写好了与七牛云对接的部分，不过scrapy爬取图片的时候遇到点坑，准备明天再弄
11.7|写好了与七牛云对接的部分，不过scrapy爬取图片的时候遇到点坑，准备明天再弄
11.8|1、无</br>2、爬虫数据先预先爬取到测试数据库，然后再进行排序放到正式数据库。今天用的时候速度很慢，所以我删掉了数据重新设置了一些参数，将最大请求书和最大线程数均调到了100，估计这样会快很多，之后进行增量爬取是直接在内存中进行排序。
11.9|无
11.10|无
11.11|无
11.12|七牛云中文件更换文件名前缀
11.13|学习scrapyd的使用，用于部署爬虫，可以很方便的对爬虫项目进行监控，添加或者删除等操作
11.14|学习scrapyd的使用，用于部署爬虫，可以很方便的对爬虫项目进行监控，添加或者删除等操作
11.15|1、将第一张图片的地址换成了https请求</br>2、找了一本不错的书《python cookbook》，一些小的例子，见识了自己之前没见过的很多实用的编程方法
11.16|易班APP抓包，完成登录过程测试
11.17|1、学习了怎么用HBuilder和mui框架做webapp</br>2、完成易班首页抓包，写了一份抓包结果的文档。</br>3、抓包请求流程大概就是先请求chd登陆，在跳转另一个地址进行验证，返回say，获取say再请求一个易班地址获取access_token，再带上这个token请求一个登陆地址（否则服务器无法将登陆账号和相关学校进行关联起来），获取文章的地址和首页数据地址相同，只是参数略有区别。
11.18|接着学习做webapp的界面
11.19|1、在原先的爬虫基础上面增加易班的登陆，爬取文章列表逻辑，还没有写文章的匹配规则</br>2、继续学习做mui框架，差不多算做完了第一个页面
11.20|完善了易班文章爬虫的逻辑，下一页的解析，爬虫的结束，以及增量爬取
11.21|无
11.22|分析易班的文章，估计网页做了一些类似加密的东西，无法直接获取到数据。于是决定使用另一个可以执行js代码的模块（requests-html）去爬取，需要安装chrome，然后今天一晚上就折腾依赖库去了。另外学了通过ldd和repoquery来解决依赖问题
11.23|1、今天安装了docker和splash（一个可以渲染js的引擎），配合scrapy使用比较方便的抓取动态网页</br>2、易班爬虫的代码基本上算完成了，但是请求的逻辑似乎做出了一点更改，因为跟以前一样的请求得到的是一个错误的结果。明天再抓包分析一下。不知道是我的账号出问题了（这是个测试账号，没有绑定手机号和学校认证），还是暂时易班的一卡通登陆接口出了问题
11.24|本来文章是可以不用携带token直接进行请求的，但是在分析的时候发现还是会自动去生成cookies，但这个过程比较麻烦，后来就又去抓了一下手机的包，然后发现带上token和一个比较容易生成的cookies就可以了，代码很简单，等明天有网了才可以进行测试。
11.25|完成易班新闻爬虫
11.26~11.30|无
12.1|看asyncio的文档，学习协程模块
12.2|继续看协程库的文档，看到协程流对象部分，学习写协程的服务器和客户端以及一个简单的获取http请求头的服务。
12.3|1、继续看协程里面的锁，事件对象，信号量。学习了在协程中创建子进程以及与子进程通信的方法。</br>2、招聘的写完了一个，另一个明天再弄完。再之后就要抓紧时间复习了                                                                                           
12.4~1.12|无--------（一个多月基本上没有好好敲代码了，进度为无。中间偶尔写一下也是在忙之前的事情，总算是可以好好敲代码了）
1.13|1、今天将会话管理重新写了一遍，cookies持续化存储</br>2、提取关键词先看了看结巴分词，里面有两种算法，基于tf-idf算法以及TextRank算法的关键词提取
1.14|将成绩查询逻辑爬虫部分逻辑完成，代理部分有待解决。
1.15|1、修改成绩查询后端，使用supervisor进行监控和nginx做负载均衡。2、进行易班手机APP抓包以及文章发布的接口
1.16|无
1.17|1、完成图书馆的接口，通过supervisor+tornado进行编写</br>2、完成会话管理，cookies存储在redis中，不过cas认证部分的代码是用requests写的，会阻塞，虽然感觉还好，但是也有待优化。
1.18|修改了易班的会话管理，准备全部通过类似图书馆进行cookies的管理，并且编写了一套简单的api
1.19|看了asyncio的文档，后面的草草看完了，另外看了一下treq已经aiohttp的文档，感觉很怀念requests，尝试用asyncio构建一套类似requests的api的协程爬虫模块，暂时不知道是否可行。后期如果完成，将用这个对阻塞代码进行优化，如果做不出来，那就使用treq或者aiohttp进行
1.20|构建一个很完善的框架或者库现在的知识储备还不无法做到，所以目前使用了tornado的异步客户端AsyncHTTPClient，并且进行了进一步的封装，使用更加简便
1.21|1、今天将图书馆的bug进行修复，主要是部分账号在少数情况下会请求验证码</br>2、另外将图书馆的需要增加的内容完成，也就是历史借阅情况和违章缴款两个部分
1.22|1、修复图书馆没有进行cookies验证的bug</br>2、修改图书馆爬虫返回的数据格式，虽然前端变得方便了，但是数据量变多，而且速度变慢了
1.23|今天好像没有啥进度
1.24|今天把易班的投票部分做了一半，也就是发布投票，进行投票，点赞，另外还需要进行的就是进行完善以及投票这个过程的管理
1.25|做的事情比较少，做了一下易班的投票，点赞，评论部分
1.26|易班投票评论点赞部分基本上完成，明天可以开始做手机端上面的同步操作了
1.27|1、完成易班web端的投票点赞和评论</br>2、开始继续学习数据分析，首先将之前看的逻辑回归梯度下降重新看了一遍，然后将以前不懂的数学推导推导了一遍，感觉有点难度，另外准备开始自然语言处理的学习，嗯。。。这两个应该算是同步进行的
1.28|1、跟着做了一个新的逻辑回归案例（信用卡欺诈），学习了下采样，以及过采样使用的smote算法。</br>2、看了决策树算法的使用
