日期|工作学习情况
:-:|:-
8.1|今天复习了一下tornado框架和sqlachemy模块，学习使用alembic模块进行数据库迁移
8.2|今天进度可以算无，看了看python里面的上下文管理器，else块，以及刚刚开始看如何将使用协程
8.3|接着看协程，从生成器到协程的转变
8.4|1、今天学习了终止协程和异常处理，以及在协程中返回值</br>2、初步理解了一点yield from这个语言结构使用方法和原理，虽然知道它很强大，但是还是不能理解这么复杂的语法的意义
8.5|今天学习了一个用来处理并发的模块concurrent.futures，主要的两个类ThreadPoolExecutor和ProcessPoolExecutor内部维护着一个线程池或者进程池，感觉很方便，但是实际使用的时候可能封装级别较低的Threading和multiprocessing这两个模块可能更加灵活。
8.6|今日进度：无
8.7|1、今天学习了协程的使用，两个扩展包greenlet和gevent模块，使用协程实现并发</br>2、看了一下python标准库里面的协程asyncio模块，不过感觉很难，暂时还没有看懂。
8.8|今日进度：无
8.9|1、今天继续学习了asyncio模块</br>2、今天对之前线程池，进程池以及线程进程通信做了一个小结
8.10|无
8.11|1、学习了tornado框架中的websocket的使用</br>2、开始学习python的元编程，正在看动态属性和特性
8.12|1、继续学习元编程，创建描述符</br>2、正在学习shell编程
8.13|1、学习创建元类，以及与普通类的关系</br>2、复习了tornado异步的使用
8.14|1、学习shell编程，if，for，while语句，按照视频写了个简单的MySQL数据库自动备份脚本</br>2、学习了jQuery中使用Ajax与tornado进行数据交互
8.15|无
8.16|学习shell编程，学习了until，case，select语句以及数组和函数，学习sed，grep，awk以及find的基本使用
8.17|学习使用Supervisor来监控tornado进程
8.18|1、学习numpy的基本使用</br>2、学习使用pandas进行数据处理
8.19|1、学习pandas里面一些处理数据的操作</br>2、今天开始学习matplotlib
8.20|今日进度：继续学习了一点点matplotlib进行数据可视化
8.21~9.1|进度几乎为无
9.2|今日进度：学习一个对matplotlib进一步封装的模块seaborn
9.3|今日进度：1、测试了一下图书馆，在学校也没办法访问，明天再用校园网试一下</br>2、跟着视频学习了一些线性回归方面的知识
9.4~9.12|今日进度：几乎为无，没怎么记录
9.13|1、今天尝试写了异步代码替换之前使用requests库写的阻塞代码，之前的代码完全没有用到tornado的异步特性，对于性能没有什么提升。</br>2、重新看了看tornado里面关于协程的使用以及python中新加入的关键词async和await
9.15|完成部署，在服务器创建了一个拥有sudo权限的用户py，编译安装了python3.7和一些附件，以及使用python2安装了supervisor监控tornado进程，修改了nginx配置文件，增加了upstream和server用来做tornado的负载均衡。使用python2安装了virtualenv来创建虚拟环境
9.16|无
9.17|1、学习了一些关于代码检查代码测试的内容</br>2、看看了逻辑回归算法的讲解视频
9.18|今日进度：无
9.19|今日进度：折腾了一天的win0子系统上面的Linux，感觉win10的子系统有点坑，入门学Linux还是挺方便的，另外学了一些网络测试命令
9.20|学习基本的设计模式，使用python实现单例模式和mixin模式，在python中应该可以很方便的创建一个可扩展的动态生成不同实例的代码，不过暂时遇到一个小bug，等待解决
9.21|今日进度：跟着视频做了一个梯度下降求解线性回归的案例
9.22|今日进度：看了一遍一个简易版的scrapy的中文翻译，使用爬虫框架感觉简直是一应俱全，很强大很方便。今天主要学习里面的操作流程和css选择器、xpath以及它的全局命令
9.23|1、看scrapy的官方文档，学习了几种不同的爬虫的使用、将爬取的数据写入数据库以及一个非常好用的链接提取器。</br>2、初步分析了一下先锋家园这个网站，感觉很适合练手，比较整齐，可能个别页面需要更改爬取的规则，准备直接用框架将所有板块的链接提取出来，就可以直接爬新闻的链接了，在框架里面感觉想要什么就有什么，提取链接，分页，以及文本内容抓取这些都很方便。
9.24|学习scrapy的文档，今天爬取先锋家园所有版块的链接，然后准备进行进一步的数据抓取，但是似乎在这个地方遇到了一个问题，框架在提取链接的时候自动进行了深度遍历，然后当我希望进行再次爬取的时候会失败，但是当我在请求中设置参数使得可以重复爬取的时候，这个时候又会陷入爬行循环，会重复好几次我所做的工作，或许这是其他失误操作所导致。。。。。。在写今日进度的时候问题刚好解决，但是有些问题还是应该继续深入框架
9.25|今日进度：先锋家园爬虫爬取解析部分代码基本完成，正在增加一个redis的过滤操作和将数据写入MySQL数据库
9.26|写完了先锋家园的爬虫，但是由于最后我的ip被限制了（本来还可以用的，可能测试的次数太多了，被制裁了），估计235和236两个服务器也是由于访问次数过多所以造成的无法访问学校网站。明天写个代理应该就可以了
9.27|无
9.28|先锋家园爬虫已经完成，现在先用我的数据库进行测试，可以了的话就可以直接爬到235的数据库里面了
9.29|在236上面装了个redis服务，调试了一下配置文件以及redis和mysql的连接情况，解决在后台运行爬虫的问题，解决了一个配置日志输出文件的小问题，爬虫正常运行
9.30~10.7|国庆七天没有做什么，深入看了一遍scrapy的文档，各种中间件以及配置（不过感觉看完就忘得差不多了。。），另外在GitHub上面找到了一个爬取代理的项目，还是很不错，用tornado以及scrapy结合写的一个项目，部署到了我的服务器上面，感觉深入理解理解里面tornado和scrapy结合的地方很有好处
10.8|今天将爬虫数据库去重以及字段修改完了，改进了一下代码，还没有进行最终测试
10.9|先锋家园爬虫测试完毕，优化了部分代码，正在将其加入到定时任务中
10.10|无
10.11|1、先锋家园爬虫完毕，但是更改了tag和做了数据库过滤之后感觉效率低了很多。</br>2、信息门户爬虫刚开始，使用scrapy的CookiesMiddleware中间件使用cookie</br>找到了一本不错的书《learning+scrapy》，大概可以找到嵌入web以及合理使用性能的方法
10.12~10.13|1、先锋家园爬虫完毕</br>2、信息门户爬虫完毕</br>3、对于爬虫框架的使用还不够熟练以及对于异步框架twisted使用还不会，代码还需要优化，另外由于可能由于数据库的原因造成效率低，这个地方也需要优化
10.14|1、改了改之前爬虫的bug，一个由于title没有做转义导致的错误和一个由于author太长导致的错误以及在信息门户的新闻有部分会跳转到其他链接，这个直接在框架里面设置为禁止访问了。另外有一个bug，似乎每次爬虫爬取文章数量总是预计的两倍，这个有待解决。</br>2、继续看scrapy的文档和《learning scrapy》，学习使用Item Loader来处理响应，有助于简化代码
10.15|无
10.16|1、看scrapy的文档，学习load item的使用
10.17|学习itemloader，用来提取数据，使得爬取和解析分离，逻辑更加清楚，另外写了一篇笔记。
10.18|正在写爬取学校官网新闻，提取链接。
10.19|写完了官网的爬取和解析部分，数据库表和字段有更改，正在更改与数据库相关的逻辑
10.20|无
10.21|修改了爬虫与数据库交互部分的逻辑，以及一些小bug
10.22|增加了爬虫增量爬取的功能，设置了延迟，爬取的速度降低，但是由于是一直跑着，所以速度影响不大
10.23|1、学校官网爬虫，先锋家园爬虫，信息门户爬虫三个都增加了增量爬取（准确说是实时更新爬取的功能），而且已经正常使用</br>2、接下来准备继续优化当前爬虫代码并且开始阅读异步框架twisted的文档，另外准备开始写易班官网的爬虫
10.24|修改了部分爬虫有问题的地方，还没来得及测试
10.25|解决了爬虫几个小问题，调换了爬取的两个时间，补全文章图片链接，加上了tags表
10.26|改了改爬虫出现的问题，但是或许还是有问题，而且令人感到奇怪的是爬虫爬取的效率比之前差了很多，这个还要继续找原因
10.27|无
10.28|无
10.29|无
10.30|无
10.31|今日进度：想办法让爬虫能够按照顺序插入到数据库，由于框架的限制，无法从调度引擎入手，尝试写了个扩展，用于执行预定好的顺序，暂时还没有成功，用中间件去做可能会更好。
10.1|看了一遍一个关于分布式爬虫的库scrapy-redis的源码，希望能够找到调用scrapy引擎的方法和自定义队列的方法。不过还不是很懂。
10.2|无
10.3|1、优化了以前的两个爬虫</br>2、暂时不准备对scrapy框架进行改造。目前准备直接将所有数据放在内存里面进行排序，然后再插入数据库，这样做的话很慢，不再有异步的优势，不过这是最简单最直接的办法。
10.4|今日进度：优化了信息门户的爬虫
10.5|1、完成了爬虫</br>2、注册了一个七牛云账号，暂时还在认证中，这个接口还没有写，准备等这个好了再重新爬数据
10.6|写好了与七牛云对接的部分，不过scrapy爬取图片的时候遇到点坑，准备明天再弄
10.7|写好了与七牛云对接的部分，不过scrapy爬取图片的时候遇到点坑，准备明天再弄
10.8|1、无</br>2、爬虫数据先预先爬取到测试数据库，然后再进行排序放到正式数据库。今天用的时候速度很慢，所以我删掉了数据重新设置了一些参数，将最大请求书和最大线程数均调到了100，估计这样会快很多，之后进行增量爬取是直接在内存中进行排序。
10.9|无
10.10|无
10.11|无
10.12|七牛云中文件更换文件名前缀
10.13|学习scrapyd的使用，用于部署爬虫，可以很方便的对爬虫项目进行监控，添加或者删除等操作
10.14|学习scrapyd的使用，用于部署爬虫，可以很方便的对爬虫项目进行监控，添加或者删除等操作
10.15|1、将第一张图片的地址换成了https请求</br>2、找了一本不错的书《python cookbook》，一些小的例子，见识了自己之前没见过的很多实用的编程方法
10.16|易班APP抓包，完成登录过程测试
